import numpy as np
import tensorflow as tf
from tensorflow.keras import datasets, models, layers, losses
import matplotlib.pyplot as plt

# Перевірка доступності GPU
print("GPU доступний:", tf.test.is_gpu_available())

# 1. Завантаження та підготовка даних
def load_and_preprocess_data():
    # Завантаження датасету MNIST
    (train, train_labels), (test, test_labels) = datasets.mnist.load_data()

    # Нормалізація значень пікселів до діапазону [0, 1]
    train = train.astype("float32") / 255.0
    test = test.astype("float32") / 255.0

    print("Розмір до трансформації:", train.shape, test.shape)

    # Додавання каналу до зображень
    train = np.expand_dims(train, -1)
    test = np.expand_dims(test, -1)

    print("Розмір після трансформації:", train.shape, test.shape)

    # Перетворення міток у one-hot формат
    train_labels = tf.one_hot(train_labels, 10)
    test_labels = tf.one_hot(test_labels, 10)

    return (train, train_labels), (test, test_labels)

# 2. Візуалізація даних
def visualize_data(train_data, num_samples=25):
    plt.figure(figsize=(10, 10))
    for i in range(num_samples):
        plt.subplot(5, 5, i+1)
        plt.imshow(train_data[i, :, :, 0], cmap='gray')
        plt.axis('off')
    plt.suptitle('Приклади зображень з MNIST')
    plt.show()

# 3. Параметри моделі
class Hyperparams:
    batch_size = 512
    kernel_size = 3
    n_epochs = 5  # Збільшено до 5 для кращого навчання
    optimizer = "adam"

# 4. Створення моделі
def create_model():
    model = models.Sequential([
        layers.InputLayer(input_shape=(28, 28, 1)),
        layers.Conv2D(16, Hyperparams.kernel_size, activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(32, Hyperparams.kernel_size, activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dropout(0.2),  # Додано Dropout для регуляризації
        layers.Dense(64, activation="relu"),  # Збільшено розмір Dense шару
        layers.Dense(10)  # Вихідний шар для 10 класів
    ])
    return model

# 5. Основний код
def main():
    # Завантаження даних
    (train, train_labels), (test, test_labels) = load_and_preprocess_data()

    # Візуалізація
    visualize_data(train)

    # Створення та компіляція моделі
    model = create_model()
    model.summary()

    model.compile(
        optimizer=Hyperparams.optimizer,
        loss=losses.CategoricalCrossentropy(from_logits=True),
        metrics=["accuracy"]
    )

    # Навчання моделі з callbacks
    callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)
    ]

    report = model.fit(
        train,
        train_labels,
        epochs=Hyperparams.n_epochs,
        batch_size=Hyperparams.batch_size,
        validation_data=(test, test_labels),
        callbacks=callbacks
    )

    # Візуалізація результатів
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(report.history['accuracy'], label='Точність (тренування)')
    plt.plot(report.history['val_accuracy'], label='Точність (валідація)')
    plt.xlabel('Епоха')
    plt.ylabel('Точність')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(report.history['loss'], label='Втрати (тренування)')
    plt.plot(report.history['val_loss'], label='Втрати (валідація)')
    plt.xlabel('Епоха')
    plt.ylabel('Втрати')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    # Оцінка моделі
    test_loss, test_acc = model.evaluate(test, test_labels, verbose=0)
    print(f"\nРезультати на тестовому наборі:")
    print(f"Тестові втрати: {test_loss:.4f}")
    print(f"Тестова точність: {test_acc:.4f}")

if __name__ == "__main__":
    main()
